- Class: meta
  Course: scRNA-seq
  Lesson: Single Cell Clustering
  Author: Matthew Moss
  Type: Standard
  Organization: 
  Version: 2.4.5

- Class: text
  Output: The most powerful aspect of single cell RNA-sequencing is the ability to explore the different cell types contained in your data

- Class: text
  Output: In order to do that, you have to cluster all of the cells in your data. Doing this will allow you to explore the differences between groups of cells that are identified to be the same type
  
- Class: text
  Output: There are several parameters to consider before clustering your data. The main ones we'll discuss are the amount of variability in the data to take into consideration (representated by how many principle components you use in your clustering), and the number of cells in your dataset
  
- Class: text
  Output: In order to determine the number of principle components we'll use, let's create an Elbow Plot. This plots the each principle component in order based on the magnitude of variance it represents
  
- Class: cmd_question
  Output: Let's take a look at our elbow plot using the ElbowPlot() function on our pbmc data (already in your environment)
  CorrectAnswer: ElbowPlot(pbmc)
  AnswerTests: omnitest(correctExpr('ElbowPlot(pbmc)'))
  Hint: Run ElbowPlot() on the pbmc variable!
  
- Class: text
  Output: When looking at this plot, we deteremine our cutoff based on where the curve of the points starts to flatten. On this graph it seems to be around PC 9, so we'll consider the first 10 PCs in our clustering
  
- Class: text
  Output: With this information in hand, and the knowledge that our object contains approximately 2700 cells, we can begin our clustering
  
- Class: text
  The number of cells tells us what resolution we can use, which determines the number of clusters that will be detected in the data. For larger datasets it is possible to use a higher resolution, because more cells allows us to identify less common cell types. For around 3000 cells, Seurat recommendes a resolution between 0.4 and 1.2
  
- Class: text
  Output: The clustering process occurs in two steps. First, we use the FindNeighbors() function to calculate the distance between our cells in the PC space. Next, the FindClusters() function uses those distances to find which cells are closest to each other, and calls those cells a cluster. There are more complicated algorithms underlying this, but that's the basic understanding
  
- Class: text
  Output: Now let's try clustering our data!

- Class: cmd_question
  Output: Run the FindNeighbors() function on our pbmc data. Set the dims flag equal to 1:10 to use 10 principle component dimensions, and save it to the pbmc object
  CorrectAnswer: pbmc <- FindNeighbors(pbmc, dims = 1:10)
  AnswerTests: omnitest(correctExpr('pbmc <- FindNeighbors(pbmc, dims = 1:10)'))
  Hint: Make sure to set dims = 1:10!

- Class: cmd_question
  Output: Now run the FindClusters() function on our pbmc data. Set the resolution flag equal to 0.5 since we have less the 3000 cells, and save it to the pbmc object
  CorrectAnswer: pbmc <- FindClusters(pbmc, resolution = 0.5)
  AnswerTests: omnitest(correctExpr('pbmc <- FindClusters(pbmc, resolution = 0.5)'))
  Hint: Make sure to set resolution = 0.5!
  
- Class: text
  Output: Now let's visualize our data and see how many clusters we've identified! Let's first see how those clusters look in the priciple component space
  
- Class: text
  Output: Use the DimPlot() function to view our newly clustered data. Set the reduction flag to "pca"
  CorrectAnswer: DimPlot(pbmc, reduction = "pca")
  AnswerTests: omnitest(correctExpr('DimPlot(pbmc, reduction = "pca")'))
  Hint: Make sure to set the reduction = "pca"
  
- Class: text
  Output: As you can see, the clusters correspond pretty well to the 3 major groups we say before, but two of those groups have heterogeneity in them. The different cell types don't seem to overlap in the PC space, and they would have been hard to see on that original graph. The power of this clustering is its ability to identify the differences in those larger groups!
  
- Class: text
  Output: Finally, let's re-project our data into another common dimensional space. For this lesson we will use the UMAP projection, though there are many others you could use!

- Class: cmd_question
  Output: Use the RunUMAP() function to project your data into the UMAP space. This function also requires you to specify how many principle components you want to use, so we'll use the same 10 as before. Set your dims flag equal to 1:10 and save it all to you pbmc object
  CorrectAnswer: pbmc <- RunUMAP(pbmc, dims = 1:10)
  AnswerTests: omnitest(correctExpr('pbmc <- RunUMAP(pbmc, dims = 1:10)'))
  Hint: Make sure to set dims = 1:10!
  
- Class: text
  Output: Now we can view our data in that new projection, again using the DimPlot() function
  
- Class: cmd_question
  Output: Run the DimPlot() function on your pbmc object again, but this time set your reduction flag equal to "umap"
  CorrectAnswer: DimPlot(pbmc, reduction = "umap")
  AnswerTests: omnitest(correctExpr('DimPlot(pbmc, reduction = "umap")'))
  Hint: Don't forget to set your reduction = "umap"!
  
- Class: text
  Output: Now you should see your 9 clusters separated into the same groups as on your pca projection, but this reduction should look much cleaner!
  
- Class: text
  Output: You have now clustered your data into 9 cell types. In our next lesson, we'll learn how to explore your clusters and find markers that help you identify what types of cell each cluster corresponds to. See you then!